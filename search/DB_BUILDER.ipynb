{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity News Timeline Database Builder\n",
    "\n",
    "This notebook builds a timeline database mapping celebrities to significant news events across the years 2004-2025. It leverages the Google Custom Search JSON API to find major news articles for each celebrity and organizes them by year.\n",
    "\n",
    "**Purpose:** The generated database can be used as a basis for analyzing public sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Set\n",
    "from urllib.parse import quote_plus, urlparse\n",
    "from google import GoogleSearchAPI, RateLimitExceededError, GoogleSearchError\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "load_dotenv(os.path.join(parent_dir, '.env'))\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "search_engine_id = os.environ.get(\"GOOGLE_CSE_ID\") #load env variables\n",
    "\n",
    "#const\n",
    "START_YEAR = 2004\n",
    "END_YEAR = 2025\n",
    "MAX_ARTICLES_PER_YEAR = 3 #max articles to find per year per celebrity\n",
    "CACHE_FILE = \"celeb_timeline_cache.json\" #cache Google Search results\n",
    "OUTPUT_DB_FILE = \"celebrity_timeline_db.json\" #output file\n",
    "INPUT_CSV_FILE = \"../celeb_data.csv\" #input file with celebrity names\n",
    "\n",
    "REPUTABLE_DOMAINS = {\n",
    "    \"nytimes.com\", \"washingtonpost.com\", \"wsj.com\", \"reuters.com\", \"apnews.com\",\n",
    "    \"bbc.com\", \"bbc.co.uk\", \"cnn.com\", \"nbcnews.com\", \"abcnews.go.com\", \"cbsnews.com\",\n",
    "    \"theguardian.com\", \"npr.org\", \"time.com\", \"forbes.com\", \"bloomberg.com\",\n",
    "    \"economist.com\", \"latimes.com\", \"usatoday.com\", \"politico.com\", \"thehill.com\",\n",
    "    \"foxnews.com\", \"cnbc.com\", \"businessinsider.com\", \"vox.com\", \"huffpost.com\",\n",
    "    \"variety.com\", \"hollywoodreporter.com\", \"ew.com\", \"rollingstone.com\", \"billboard.com\",\n",
    "    \"espn.com\", \"sports.yahoo.com\", \"si.com\", \"people.com\", \"eonline.com\", \"tmz.com\"\n",
    "}\n",
    "\n",
    "MAJOR_EVENT_KEYWORDS = [\n",
    "    \"award\", \"win\", \"won\", \"nominated\", \"nomination\", \"oscar\", \"grammy\", \"emmy\", \"golden globe\",\n",
    "    \"marriage\", \"divorce\", \"wedding\", \"engaged\", \"engagement\", \"child\", \"baby\", \"born\",\n",
    "    \"scandal\", \"controversy\", \"lawsuit\", \"legal\", \"court\", \"trial\", \"arrested\", \"prison\",\n",
    "    \"movie\", \"film\", \"album\", \"song\", \"release\", \"premiere\", \"debut\", \"launch\",\n",
    "    \"milestone\", \"achievement\", \"breakthrough\", \"record\", \"bestseller\", \"box office\",\n",
    "    \"died\", \"death\", \"accident\", \"health\", \"illness\", \"surgery\", \"hospitalized\",\n",
    "    \"political\", \"campaign\", \"election\", \"president\", \"appointed\", \"named\", \"ceo\",\n",
    "    \"founded\", \"company\", \"business\", \"startup\", \"investment\", \"philanthropy\", \"charity\",\n",
    "    \"comeback\", \"return\", \"retirement\", \"memoir\", \"autobiography\", \"book\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration: API Client Initialization\n",
    "\n",
    "This cell retrieves the Google API credentials from environment variables and initializes the `GoogleSearchAPI` client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_client = GoogleSearchAPI(\n",
    "    api_key=api_key,\n",
    "    search_engine_id=search_engine_id,\n",
    "    max_retries=5,\n",
    "    retry_delay=3,\n",
    "    requests_per_day=9999,  #daily limit\n",
    "    requests_per_second=5   #rate limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "These functions handle tasks like caching, filtering search results based on relevance and source reputation, and saving/loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(cache_file):\n",
    "    \"\"\"Loads cached search results from a JSON file\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def save_cache(cache, cache_file):\n",
    "    \"\"\"Saves the current cache state to a JSON file\"\"\"\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, indent=2)\n",
    "        \n",
    "def get_cache_key(celebrity, year):\n",
    "    \"\"\"Generates a unique cache key for a celebrity and year.\"\"\"\n",
    "    return f\"{celebrity.lower().replace(' ', '_')}_{year}\"\n",
    "\n",
    "def is_major_event(title, snippet):\n",
    "    \"\"\"Determines if an article likely represents a major event based on keywords.\"\"\"\n",
    "    content = (title + \" \" + snippet).lower()\n",
    "    has_major_keyword = any(keyword in content for keyword in MAJOR_EVENT_KEYWORDS)\n",
    "    return has_major_keyword\n",
    "\n",
    "def is_from_reputable_source(url):\n",
    "    \"\"\"Checks if the URL's domain is in the predefined list.\"\"\"\n",
    "    domain = urlparse(url).netloc\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]\n",
    "    return domain in REPUTABLE_DOMAINS\n",
    "\n",
    "def save_database(db, output_file):\n",
    "    \"\"\"Saves the timeline database to a JSON file.\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(db, f, indent=2)\n",
    "\n",
    "def load_database(input_file):\n",
    "    \"\"\"Loads a timeline database.\"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        return None\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        db = json.load(f)\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Logic: Searching and Building Timelines\n",
    "\n",
    "These functions perform the actual search using the Google API client, filter results, and assemble the timeline data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_celebrity_year(\n",
    "    api_client,\n",
    "    celebrity,\n",
    "    year,\n",
    "    cache,\n",
    "    use_cache,\n",
    "    cache_file):\n",
    "    \"\"\"\n",
    "    Searches for major news events for a celebrity in a specific year.\n",
    "    \"\"\"\n",
    "    cache_key = get_cache_key(celebrity, year)\n",
    "\n",
    "    if use_cache and cache_key in cache:\n",
    "        cached_data = cache[cache_key]\n",
    "        return cached_data\n",
    "        \n",
    "    queries = [\n",
    "        f'\"{celebrity}\" {year} news',\n",
    "        f'\"{celebrity}\" {year} major event',\n",
    "        f'\"{celebrity}\" {year} award', \n",
    "        f'\"{celebrity}\" {year} controversy' \n",
    "    ]\n",
    "\n",
    "    found_articles = {} # {url: {details}}\n",
    "\n",
    "    num_queries_run = 0\n",
    "    for query in queries:\n",
    "        if len(found_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "            break\n",
    "\n",
    "        if num_queries_run > 0:\n",
    "            time.sleep(1.2) #small delay\n",
    "\n",
    "        try:\n",
    "            num_queries_run += 1\n",
    "            response = api_client.search_by_year(query, year)\n",
    "            results = api_client.extract_search_results(response) #list of dicts\n",
    "\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            for result in results:\n",
    "                url = result.get('link', '')\n",
    "                title = result.get('title', '')\n",
    "                snippet = result.get('snippet', '')\n",
    "\n",
    "                if not url or not title or url in found_articles:\n",
    "                    continue\n",
    "\n",
    "                if is_major_event(title, snippet) and is_from_reputable_source(url):\n",
    "                    found_articles[url] = {\n",
    "                        \"title\": title,\n",
    "                        \"link\": url,\n",
    "                        \"snippet\": snippet\n",
    "                    }\n",
    "                    if len(found_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "                        break\n",
    "\n",
    "        except RateLimitExceededError as e:\n",
    "            print(f\"RLE for {celebrity} ({year}):. Stopping searches for this celebrity.\")\n",
    "            break\n",
    "        except GoogleSearchError as e:\n",
    "            print(f\"GSE for {celebrity} ({year}): {e}. Trying next query.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    major_events_list = list(found_articles.values())\n",
    "\n",
    "    #cache results\n",
    "    if use_cache:\n",
    "        cache[cache_key] = major_events_list\n",
    "        save_cache(cache, cache_file) #save cache after each year\n",
    "\n",
    "    return major_events_list\n",
    "\n",
    "\n",
    "def build_timeline_for_celebrity(\n",
    "    api_client,\n",
    "    celebrity,\n",
    "    cache,\n",
    "    use_cache,\n",
    "    cache_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a year-by-year timeline of major news events for a single celebrity.\n",
    "    \"\"\"\n",
    "    celebrity_timeline = {}\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        yearly_events = search_celebrity_year(\n",
    "            api_client=api_client,\n",
    "            celebrity=celebrity,\n",
    "            year=year,\n",
    "            cache=cache,\n",
    "            use_cache=use_cache,\n",
    "            cache_file=cache_file\n",
    "        )\n",
    "        \n",
    "        if yearly_events:\n",
    "            celebrity_timeline[str(year)] = yearly_events # Use string keys for JSON compatibility\n",
    "\n",
    "        \n",
    "        time.sleep(0.5) \n",
    "            \n",
    "    return celebrity_timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution: Build the Complete Database\n",
    "\n",
    "This cell orchestrates the process:\n",
    "1. Loads existing cache (if any).\n",
    "2. Reads celebrity names from the input CSV file.\n",
    "3. Iterates through each celebrity, building their timeline using the functions above.\n",
    "4. Saves the cache and the final database periodically and at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 3 seconds...\n",
      "Rate limit exceeded. Retrying in 3 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\16196\\Desktop\\DATAHACKS-2025\\search\\google.py:232\u001b[0m, in \u001b[0;36mGoogleSearchAPI.search\u001b[1;34m(self, query, start, num, search_type, fields, sort, safe, cx, gl, cr, lr, rights, filter, date_restrict, exact_terms, exclude_terms, file_type, site_search, site_search_filter, link_site, or_terms, related_site, **additional_params)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Check for HTTP errors\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Parse and return the JSON response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\16196\\miniforge3\\envs\\dsc80\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyD4oFRzrXreqRm4v3ImVSgX9i0S0qRT8iU&cx=312c61f998c4e4771&q=%22Leonardo+DiCaprio%22+2004+news&start=1&num=10&safe=off&filter=0&dateRestrict=y2004",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 60\u001b[0m\n\u001b[0;32m     52\u001b[0m             save_database(timeline_db, output_db_file)\n\u001b[0;32m     56\u001b[0m     save_database(timeline_db, output_db_file)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mrun_database_build\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_CSV_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_db_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DB_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCACHE_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mrun_database_build\u001b[1;34m(api_client, input_csv, output_db_file, cache_file, use_cache, save_interval)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m celebrity \u001b[38;5;129;01min\u001b[39;00m timeline_db:\n\u001b[0;32m     40\u001b[0m      \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m#skip\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m celebrity_timeline \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_timeline_for_celebrity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcelebrity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# add to db\u001b[39;00m\n\u001b[0;32m     47\u001b[0m timeline_db[celebrity] \u001b[38;5;241m=\u001b[39m celebrity_timeline\n",
      "Cell \u001b[1;32mIn[4], line 92\u001b[0m, in \u001b[0;36mbuild_timeline_for_celebrity\u001b[1;34m(api_client, celebrity, cache, use_cache, cache_file)\u001b[0m\n\u001b[0;32m     89\u001b[0m celebrity_timeline \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(START_YEAR, END_YEAR \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     yearly_events \u001b[38;5;241m=\u001b[39m \u001b[43msearch_celebrity_year\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcelebrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcelebrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yearly_events:\n\u001b[0;32m    102\u001b[0m         celebrity_timeline[\u001b[38;5;28mstr\u001b[39m(year)] \u001b[38;5;241m=\u001b[39m yearly_events \u001b[38;5;66;03m# Use string keys for JSON compatibility\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36msearch_celebrity_year\u001b[1;34m(api_client, celebrity, year, cache, use_cache, cache_file)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     num_queries_run \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 36\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_by_year\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     results \u001b[38;5;241m=\u001b[39m api_client\u001b[38;5;241m.\u001b[39mextract_search_results(response) \u001b[38;5;66;03m#list of dicts\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\16196\\Desktop\\DATAHACKS-2025\\search\\google.py:327\u001b[0m, in \u001b[0;36mGoogleSearchAPI.search_by_year\u001b[1;34m(self, query, year, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m date_restrict \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Alternative approach: use date range for the entire year\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# start_date = f\"{year}-01-01\"\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# end_date = f\"{year}-12-31\"\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# return self.search_by_date_range(query, start_date, end_date, **kwargs)\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_restrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_restrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16196\\Desktop\\DATAHACKS-2025\\search\\google.py:243\u001b[0m, in \u001b[0;36mGoogleSearchAPI.search\u001b[1;34m(self, query, start, num, search_type, fields, sort, safe, cx, gl, cr, lr, rights, filter, date_restrict, exact_terms, exclude_terms, file_type, site_search, site_search_filter, link_site, or_terms, related_site, **additional_params)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded. Retrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_after\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 243\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_database_build(\n",
    "    api_client,\n",
    "    input_csv,\n",
    "    output_db_file,\n",
    "    cache_file,\n",
    "    use_cache = True,\n",
    "    save_interval = 5 #save db  every N celebrities\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to run the database build process.\n",
    "    \"\"\"\n",
    "    cache = load_cache(cache_file)\n",
    "\n",
    "    timeline_db = load_database(output_db_file)\n",
    "    if timeline_db is None:\n",
    "        timeline_db = {}\n",
    "\n",
    "\n",
    "\n",
    "    # celeb names\n",
    "    celebrities_to_process = []\n",
    "    \n",
    "    with open(input_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if 'Name' not in reader.fieldnames:\n",
    "            return\n",
    "        for row in reader:\n",
    "            name = row['Name'].strip()\n",
    "            if name and name not in timeline_db:\n",
    "                celebrities_to_process.append(name)\n",
    "            else:\n",
    "                pass #skip\n",
    "\n",
    "\n",
    "    #build timeline for celebs\n",
    "    processed_count = 0\n",
    "    for i, celebrity in enumerate(celebrities_to_process):\n",
    "\n",
    "        if celebrity in timeline_db:\n",
    "             continue #skip\n",
    "\n",
    "        celebrity_timeline = build_timeline_for_celebrity(\n",
    "            api_client, celebrity, cache, use_cache, cache_file\n",
    "        )\n",
    "\n",
    "        # add to db\n",
    "        timeline_db[celebrity] = celebrity_timeline\n",
    "        processed_count += 1\n",
    "\n",
    "        #save prog periodically\n",
    "        if processed_count > 0 and processed_count % save_interval == 0:\n",
    "            save_database(timeline_db, output_db_file)\n",
    "\n",
    "\n",
    "\n",
    "    save_database(timeline_db, output_db_file)\n",
    "\n",
    "\n",
    "\n",
    "run_database_build(\n",
    "    api_client=api_client,\n",
    "    input_csv=INPUT_CSV_FILE,\n",
    "    output_db_file=OUTPUT_DB_FILE,\n",
    "    cache_file=CACHE_FILE,\n",
    "    use_cache=True,\n",
    "    save_interval=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. View Results\n",
    "\n",
    "Load the generated JSON database and inspect a sample entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load db\n",
    "final_db = load_database(OUTPUT_DB_FILE)\n",
    "print(f\" {len(final_db)} celebrities.\")\n",
    "first_celebrity = list(final_db.keys())[0]\n",
    "print(f\"First celebrity: {first_celebrity}\")\n",
    "print(f\"Timeline for {first_celebrity}:\")\n",
    "print(json.dumps(final_db[first_celebrity], indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
