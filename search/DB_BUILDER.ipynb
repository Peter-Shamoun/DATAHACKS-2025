{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity News Timeline Database Builder\n",
    "\n",
    "This notebook builds a timeline database mapping celebrities to significant news events across the years 2004-2025. It leverages the Google Custom Search JSON API to find major news articles for each celebrity and organizes them by year.\n",
    "\n",
    "**Purpose:** The generated database can be used as a basis for analyzing public sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Set\n",
    "from urllib.parse import quote_plus, urlparse\n",
    "from google import GoogleSearchAPI, RateLimitExceededError, GoogleSearchError\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "load_dotenv(os.path.join(parent_dir, '.env'))\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "search_engine_id = os.environ.get(\"GOOGLE_CSE_ID\") #load env variables\n",
    "\n",
    "#const\n",
    "START_YEAR = 2004\n",
    "END_YEAR = 2025\n",
    "MAX_ARTICLES_PER_YEAR = 3 #max articles to find per year per celebrity\n",
    "CACHE_FILE = \"celeb_timeline_cache.json\" #cache Google Search results\n",
    "OUTPUT_DB_FILE = \"celebrity_timeline_db.json\" #output file\n",
    "INPUT_CSV_FILE = \"../celeb_data.csv\" #input file with celebrity names\n",
    "\n",
    "REPUTABLE_DOMAINS = {\n",
    "    \"nytimes.com\", \"washingtonpost.com\", \"wsj.com\", \"reuters.com\", \"apnews.com\",\n",
    "    \"bbc.com\", \"bbc.co.uk\", \"cnn.com\", \"nbcnews.com\", \"abcnews.go.com\", \"cbsnews.com\",\n",
    "    \"theguardian.com\", \"npr.org\", \"time.com\", \"forbes.com\", \"bloomberg.com\",\n",
    "    \"economist.com\", \"latimes.com\", \"usatoday.com\", \"politico.com\", \"thehill.com\",\n",
    "    \"foxnews.com\", \"cnbc.com\", \"businessinsider.com\", \"vox.com\", \"huffpost.com\",\n",
    "    \"variety.com\", \"hollywoodreporter.com\", \"ew.com\", \"rollingstone.com\", \"billboard.com\",\n",
    "    \"espn.com\", \"sports.yahoo.com\", \"si.com\", \"people.com\", \"eonline.com\", \"tmz.com\"\n",
    "}\n",
    "\n",
    "MAJOR_EVENT_KEYWORDS = [\n",
    "    \"award\", \"win\", \"won\", \"nominated\", \"nomination\", \"oscar\", \"grammy\", \"emmy\", \"golden globe\",\n",
    "    \"marriage\", \"divorce\", \"wedding\", \"engaged\", \"engagement\", \"child\", \"baby\", \"born\",\n",
    "    \"scandal\", \"controversy\", \"lawsuit\", \"legal\", \"court\", \"trial\", \"arrested\", \"prison\",\n",
    "    \"movie\", \"film\", \"album\", \"song\", \"release\", \"premiere\", \"debut\", \"launch\",\n",
    "    \"milestone\", \"achievement\", \"breakthrough\", \"record\", \"bestseller\", \"box office\",\n",
    "    \"died\", \"death\", \"accident\", \"health\", \"illness\", \"surgery\", \"hospitalized\",\n",
    "    \"political\", \"campaign\", \"election\", \"president\", \"appointed\", \"named\", \"ceo\",\n",
    "    \"founded\", \"company\", \"business\", \"startup\", \"investment\", \"philanthropy\", \"charity\",\n",
    "    \"comeback\", \"return\", \"retirement\", \"memoir\", \"autobiography\", \"book\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration: API Client Initialization\n",
    "\n",
    "This cell retrieves the Google API credentials from environment variables and initializes the `GoogleSearchAPI` client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_client = GoogleSearchAPI(\n",
    "    api_key=api_key,\n",
    "    search_engine_id=search_engine_id,\n",
    "    max_retries=5,\n",
    "    retry_delay=3,\n",
    "    requests_per_day=9999,  #daily limit\n",
    "    requests_per_second=5   #rate limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "These functions handle tasks like caching, filtering search results based on relevance and source reputation, and saving/loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(cache_file):\n",
    "    \"\"\"Loads cached search results from a JSON file\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def save_cache(cache, cache_file):\n",
    "    \"\"\"Saves the current cache state to a JSON file\"\"\"\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, indent=2)\n",
    "        \n",
    "def get_cache_key(celebrity, year):\n",
    "    \"\"\"Generates a unique cache key for a celebrity and year.\"\"\"\n",
    "    return f\"{celebrity.lower().replace(' ', '_')}_{year}\"\n",
    "\n",
    "def is_major_event(title, snippet):\n",
    "    \"\"\"Determines if an article likely represents a major event based on keywords.\"\"\"\n",
    "    content = (title + \" \" + snippet).lower()\n",
    "    has_major_keyword = any(keyword in content for keyword in MAJOR_EVENT_KEYWORDS)\n",
    "    return has_major_keyword\n",
    "\n",
    "def is_from_reputable_source(url):\n",
    "    \"\"\"Checks if the URL's domain is in the predefined list.\"\"\"\n",
    "    domain = urlparse(url).netloc\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]\n",
    "    return domain in REPUTABLE_DOMAINS\n",
    "\n",
    "def save_database(db, output_file):\n",
    "    \"\"\"Saves the timeline database to a JSON file.\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(db, f, indent=2)\n",
    "\n",
    "def load_database(input_file):\n",
    "    \"\"\"Loads a timeline database.\"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        return None\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        db = json.load(f)\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Logic: Searching and Building Timelines\n",
    "\n",
    "These functions perform the actual search using the Google API client, filter results, and assemble the timeline data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_celebrity_year(\n",
    "    api_client,\n",
    "    celebrity,\n",
    "    year,\n",
    "    cache,\n",
    "    use_cache,\n",
    "    cache_file):\n",
    "    \"\"\"\n",
    "    Searches for major news events for a celebrity in a specific year.\n",
    "    \"\"\"\n",
    "    cache_key = get_cache_key(celebrity, year)\n",
    "\n",
    "    if use_cache and cache_key in cache:\n",
    "        cached_data = cache[cache_key]\n",
    "        return cached_data\n",
    "        \n",
    "    queries = [\n",
    "        f'\"{celebrity}\" {year} news',\n",
    "        f'\"{celebrity}\" {year} major event',\n",
    "        f'\"{celebrity}\" {year} award', \n",
    "        f'\"{celebrity}\" {year} controversy' \n",
    "    ]\n",
    "\n",
    "    found_articles = {} # {url: {details}}\n",
    "\n",
    "    num_queries_run = 0\n",
    "    for query in queries:\n",
    "        if len(found_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "            break\n",
    "\n",
    "        if num_queries_run > 0:\n",
    "            time.sleep(1.2) #small delay\n",
    "\n",
    "        try:\n",
    "            num_queries_run += 1\n",
    "            response = api_client.search_by_year(query, year)\n",
    "            results = api_client.extract_search_results(response) #list of dicts\n",
    "\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            for result in results:\n",
    "                url = result.get('link', '')\n",
    "                title = result.get('title', '')\n",
    "                snippet = result.get('snippet', '')\n",
    "\n",
    "                if not url or not title or url in found_articles:\n",
    "                    continue\n",
    "\n",
    "                if is_major_event(title, snippet) and is_from_reputable_source(url):\n",
    "                    found_articles[url] = {\n",
    "                        \"title\": title,\n",
    "                        \"link\": url,\n",
    "                        \"snippet\": snippet\n",
    "                    }\n",
    "                    if len(found_articles) >= MAX_ARTICLES_PER_YEAR:\n",
    "                        break\n",
    "\n",
    "        except RateLimitExceededError as e:\n",
    "            print(f\"RLE for {celebrity} ({year}):. Stopping searches for this celebrity.\")\n",
    "            break\n",
    "        except GoogleSearchError as e:\n",
    "            print(f\"GSE for {celebrity} ({year}): {e}. Trying next query.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    major_events_list = list(found_articles.values())\n",
    "\n",
    "    #cache results\n",
    "    if use_cache:\n",
    "        cache[cache_key] = major_events_list\n",
    "        save_cache(cache, cache_file) #save cache after each year\n",
    "\n",
    "    return major_events_list\n",
    "\n",
    "\n",
    "def build_timeline_for_celebrity(\n",
    "    api_client,\n",
    "    celebrity,\n",
    "    cache,\n",
    "    use_cache,\n",
    "    cache_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a year-by-year timeline of major news events for a single celebrity.\n",
    "    \"\"\"\n",
    "    celebrity_timeline = {}\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        yearly_events = search_celebrity_year(\n",
    "            api_client=api_client,\n",
    "            celebrity=celebrity,\n",
    "            year=year,\n",
    "            cache=cache,\n",
    "            use_cache=use_cache,\n",
    "            cache_file=cache_file\n",
    "        )\n",
    "        \n",
    "        if yearly_events:\n",
    "            celebrity_timeline[str(year)] = yearly_events # Use string keys for JSON compatibility\n",
    "\n",
    "        \n",
    "        time.sleep(0.5) \n",
    "            \n",
    "    return celebrity_timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution: Build the Complete Database\n",
    "\n",
    "This cell orchestrates the process:\n",
    "1. Loads existing cache (if any).\n",
    "2. Reads celebrity names from the input CSV file.\n",
    "3. Iterates through each celebrity, building their timeline using the functions above.\n",
    "4. Saves the cache and the final database periodically and at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_database_build(\n",
    "    api_client,\n",
    "    input_csv,\n",
    "    output_db_file,\n",
    "    cache_file,\n",
    "    use_cache = True,\n",
    "    save_interval = 5 #save db  every N celebrities\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to run the database build process.\n",
    "    \"\"\"\n",
    "    cache = load_cache(cache_file)\n",
    "\n",
    "    timeline_db = load_database(output_db_file)\n",
    "    if timeline_db is None:\n",
    "        timeline_db = {}\n",
    "\n",
    "\n",
    "\n",
    "    # celeb names\n",
    "    celebrities_to_process = []\n",
    "    \n",
    "    with open(input_csv, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if 'Name' not in reader.fieldnames:\n",
    "            return\n",
    "        for row in reader:\n",
    "            name = row['Name'].strip()\n",
    "            if name and name not in timeline_db:\n",
    "                celebrities_to_process.append(name)\n",
    "            else:\n",
    "                pass #skip\n",
    "\n",
    "\n",
    "    #build timeline for celebs\n",
    "    processed_count = 0\n",
    "    for i, celebrity in enumerate(celebrities_to_process):\n",
    "\n",
    "        if celebrity in timeline_db:\n",
    "             continue #skip\n",
    "\n",
    "        celebrity_timeline = build_timeline_for_celebrity(\n",
    "            api_client, celebrity, cache, use_cache, cache_file\n",
    "        )\n",
    "\n",
    "        # add to db\n",
    "        timeline_db[celebrity] = celebrity_timeline\n",
    "        processed_count += 1\n",
    "\n",
    "        #save prog periodically\n",
    "        if processed_count > 0 and processed_count % save_interval == 0:\n",
    "            save_database(timeline_db, output_db_file)\n",
    "\n",
    "\n",
    "\n",
    "    save_database(timeline_db, output_db_file)\n",
    "\n",
    "\n",
    "\n",
    "run_database_build(\n",
    "    api_client=api_client,\n",
    "    input_csv=INPUT_CSV_FILE,\n",
    "    output_db_file=OUTPUT_DB_FILE,\n",
    "    cache_file=CACHE_FILE,\n",
    "    use_cache=True,\n",
    "    save_interval=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. View Results\n",
    "\n",
    "Load the generated JSON database and inspect a sample entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load db\n",
    "final_db = load_database(OUTPUT_DB_FILE)\n",
    "print(f\" {len(final_db)} celebrities.\")\n",
    "first_celebrity = list(final_db.keys())[0]\n",
    "print(f\"First celebrity: {first_celebrity}\")\n",
    "print(f\"Timeline for {first_celebrity}:\")\n",
    "print(json.dumps(final_db[first_celebrity], indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
